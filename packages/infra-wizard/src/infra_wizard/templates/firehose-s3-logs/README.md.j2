# {{ service_name | upper }} - CloudWatch Logs to S3 via Firehose

Generated by Infrastructure Wizard on {{ generation_time }}

## Overview

Automated log archiving pipeline that streams CloudWatch Logs to S3 using Kinesis Firehose for {{ service_name }}.

## Resources Created

- Kinesis Firehose delivery stream
- CloudWatch log group (source)
- CloudWatch log subscription filter
{% if create_s3_bucket %}- S3 bucket for log storage
- KMS key for S3 encryption
{% endif %}- IAM roles and policies
- CloudWatch alarms (delivery rate, success)
- SSM parameters (stream name, bucket, log group)

## Architecture

```
CloudWatch Logs ({{ service_name }})
    ↓ Subscription Filter
Kinesis Firehose
    ↓ Buffering ({{ buffer_size | default(5) }}MB or {{ buffer_interval | default(300) }}s)
    ↓ Compression (GZIP)
S3 Bucket{% if create_s3_bucket %} ({{ service_name }}-{{ environment }}-logs){% else %} ({{ s3_bucket_name }}){% endif %}
    ↓ Lifecycle Policy
Standard → IA (30d) → Glacier (90d) → Deep Archive (180d){% if log_retention_days %} → Delete ({{ log_retention_days }}d){% endif %}
```

## Configuration

### Firehose Settings
- **Buffer Size**: {{ buffer_size | default(5) }} MB
- **Buffer Interval**: {{ buffer_interval | default(300) }} seconds (5 minutes)
- **Compression**: GZIP enabled
- **Format**: Partitioned by year/month/day

### Storage Configuration
{% if create_s3_bucket %}- **S3 Bucket**: Dedicated bucket ({{ service_name }}-{{ environment }}-logs)
- **Encryption**: KMS customer-managed key
- **Versioning**: Enabled
{% else %}- **S3 Bucket**: Shared bucket ({{ s3_bucket_name }})
- **Encryption**: Existing KMS key
{% endif %}- **Public Access**: Blocked

### Lifecycle Policy
- **30 days**: Transition to Standard-IA
- **90 days**: Transition to Glacier Instant Retrieval
- **180 days**: Transition to Deep Archive
{% if log_retention_days %}- **{{ log_retention_days }} days**: Permanent deletion
{% endif %}
### CloudWatch Settings
- **Log Group**: {{ log_group_name }}
- **Retention**: {{ cloudwatch_retention | default(7) }} days
- **Filter Pattern**: `{{ filter_pattern | default('(all logs)') }}`

## S3 Object Key Format

Logs are organized with the following structure:

```
s3://{{ service_name }}-{{ environment }}-logs/
  logs/
    {{ service_name }}/
      {{ environment }}/
        year=2024/
          month=01/
            day=15/
              <timestamp>-<uuid>.gz
```

**Example**:
```
s3://myapp-prod-logs/logs/myapp/prod/year=2024/month=01/day=15/2024-01-15T10-30-00-abc123.gz
```

## Error Handling

Failed records are stored separately:
```
s3://{{ service_name }}-{{ environment }}-logs/
  errors/
    {{ service_name }}/
      {{ environment }}/
        year=2024/month=01/day=15/
          processing-failed/
```

## Usage Examples

### Query Archived Logs with Athena

Create Athena table:

```sql
CREATE EXTERNAL TABLE {{ service_name }}_logs (
  timestamp STRING,
  message STRING,
  log_level STRING
)
PARTITIONED BY (
  year STRING,
  month STRING,
  day STRING
)
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
LOCATION 's3://{{ service_name }}-{{ environment }}-logs/logs/{{ service_name }}/{{ environment }}/'
TBLPROPERTIES ('has_encrypted_data'='true');

-- Load partitions
MSCK REPAIR TABLE {{ service_name }}_logs;

-- Query logs
SELECT *
FROM {{ service_name }}_logs
WHERE year='2024' AND month='01' AND day='15'
  AND log_level = 'ERROR'
ORDER BY timestamp DESC
LIMIT 100;
```

### Download Logs from S3

```bash
# List logs for specific date
aws s3 ls s3://{{ service_name }}-{{ environment }}-logs/logs/{{ service_name }}/{{ environment }}/year=2024/month=01/day=15/

# Download and decompress
aws s3 cp s3://{{ service_name }}-{{ environment }}-logs/logs/{{ service_name }}/{{ environment }}/year=2024/month=01/day=15/ . --recursive
gunzip *.gz

# View decompressed logs
cat *.json | jq '.'
```

### CloudWatch Logs Insights Query

Query logs before they're archived:

```sql
fields @timestamp, @message
| filter @message like /ERROR/
| sort @timestamp desc
| limit 100
```

## Monitoring

### CloudWatch Alarms

1. **Delivery Rate Alarm**:
   - Triggers if no records delivered for 10 minutes
   - Indicates potential pipeline issues

2. **Delivery Success Alarm**:
   - Triggers if delivery success rate < 100%
   - Indicates S3 write failures or permission issues

### CloudWatch Metrics

```bash
# View Firehose metrics
aws cloudwatch get-metric-statistics \
  --namespace AWS/Firehose \
  --metric-name DeliveryToS3.Records \
  --dimensions Name=DeliveryStreamName,Value={{ service_name }}-{{ environment }}-logs-firehose \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 300 \
  --statistics Sum

# Check delivery success
aws cloudwatch get-metric-statistics \
  --namespace AWS/Firehose \
  --metric-name DeliveryToS3.Success \
  --dimensions Name=DeliveryStreamName,Value={{ service_name }}-{{ environment }}-logs-firehose \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 300 \
  --statistics Average
```

### Firehose Logs

Monitor Firehose delivery logs:

```bash
# View Firehose delivery logs
aws logs tail /aws/kinesisfirehose/{{ service_name }}-{{ environment }}-logs-firehose --follow

# Query for errors
aws logs filter-log-events \
  --log-group-name /aws/kinesisfirehose/{{ service_name }}-{{ environment }}-logs-firehose \
  --filter-pattern "ERROR" \
  --start-time $(date -u -d '1 hour ago' +%s)000
```

## Cost Optimization

### Data Transfer Costs
- **CloudWatch → Firehose**: $0.50 per GB
- **Firehose → S3**: $0.029 per GB
- **S3 Storage**: Tiered (Standard → IA → Glacier → Deep Archive)

### Buffer Settings Impact
- Larger buffer size → Less frequent S3 writes → Lower cost
- Longer buffer interval → More delay → Lower cost
- **Current**: {{ buffer_size | default(5) }}MB / {{ buffer_interval | default(300) }}s (balanced)

### Recommended Settings by Environment
```yaml
dev:
  buffer_size: 1MB (faster delivery, higher cost)
  retention: 7 days

staging:
  buffer_size: 5MB (default)
  retention: 14 days

prod:
  buffer_size: 10MB (cost optimized)
  retention: 90 days
```

## Security Features

- 🔒 KMS encryption (in-transit and at-rest)
- 🛡️ IAM role-based access (least privilege)
- 🚫 S3 public access blocking
- 📝 CloudWatch delivery logs
- ⚠️ Automated monitoring alarms

## Troubleshooting

### No logs in S3

1. **Check subscription filter**:
```bash
aws logs describe-subscription-filters \
  --log-group-name {{ log_group_name }}
```

2. **Verify IAM permissions**:
```bash
# Check Firehose role
aws iam get-role --role-name {{ service_name }}-{{ environment }}-logs-firehose-*

# Check CloudWatch Logs role
aws iam get-role --role-name {{ service_name }}-{{ environment }}-cwl-to-fh-*
```

3. **Review Firehose logs**:
```bash
aws logs tail /aws/kinesisfirehose/{{ service_name }}-{{ environment }}-logs-firehose
```

### High Firehose costs

1. **Reduce buffer interval**: Batch more records per S3 write
2. **Increase buffer size**: Wait longer before writing
3. **Add filter pattern**: Only archive important logs
4. **Reduce CloudWatch retention**: Lower to 1-3 days

### Logs not searchable in Athena

1. **Verify partition structure**:
```bash
aws s3 ls s3://{{ service_name }}-{{ environment }}-logs/logs/{{ service_name }}/{{ environment }}/
```

2. **Repair partitions**:
```sql
MSCK REPAIR TABLE {{ service_name }}_logs;
```

3. **Check compression format**: Ensure Athena supports GZIP

## Best Practices

### Filter Patterns
```bash
# Only ERROR and FATAL logs
[timestamp, request_id, level = ERROR || level = FATAL, ...]

# Exclude health checks
[timestamp, request_id, level, message != *health*]

# Specific log patterns
{ $.level = "ERROR" || $.status_code >= 500 }
```

### Performance Tuning
- **Buffer Size**: Balance between latency and cost
- **Compression**: GZIP provides 5-10x reduction
- **Partitioning**: Year/Month/Day supports efficient queries

### Athena Query Optimization
- Always filter by partitions (year/month/day)
- Use columnar formats (Parquet) for large-scale analytics
- Create views for common queries

## Integration Examples

### Application Configuration

**Node.js (Winston)**:
```javascript
const winston = require('winston');
const CloudWatchTransport = require('winston-cloudwatch');

const logger = winston.createLogger({
  transports: [
    new CloudWatchTransport({
      logGroupName: '{{ log_group_name }}',
      logStreamName: `${process.env.HOSTNAME}-${Date.now()}`
    })
  ]
});
```

**Python (boto3)**:
```python
import boto3
import logging

client = boto3.client('logs')
logger = logging.getLogger()

handler = watchtower.CloudWatchLogHandler(
    log_group='{{ log_group_name }}',
    stream_name=f'{socket.gethostname()}-{int(time.time())}'
)
logger.addHandler(handler)
```

## Tags

All resources are tagged with:
- Service: {{ service_name }}
- Environment: {{ environment }}
- ManagedBy: Terraform
- Owner: platform-team
- CostCenter: engineering

## Next Steps

1. Verify log delivery to S3
2. Set up Athena table for querying
3. Configure CloudWatch alarms notifications
4. Review and optimize buffer settings
5. Implement log filtering if needed

## Support

For issues or questions, contact the platform team.

---

🤖 Generated with Infrastructure Wizard
